{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets transformers numpy\n",
    "import torch, datasets, transformers\n",
    "import pandas as pd\n",
    "\n",
    "# set up Google Drive access\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "\n",
    "FILE = \"Your Directory Here/subfolder/discord.csv\" # replace with the directory in your My Drive of the file you created in preprocess.py\n",
    "df = pd.read_csv('gdrive/My Drive/'+FILE)\n",
    "dataset = datasets.Dataset.from_pandas(df)\n",
    "# if you have a lot of data, or not a lot of time, you can do something like:\n",
    "# dataset = datasets.Dataset.from_pandas(df.sample(2000))\n",
    "# replace 2000 with something that works on your hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a train-test split with 10% of data used for testing\n",
    "dataset = dataset.train_test_split(test_size=0.1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next will be the tokenizer. Here is an example of tokenization:\n",
    "\n",
    "\"I see the Apple store but I don't see any apples\"\n",
    "\n",
    "becomes\n",
    "\n",
    "`\"I\", \"see\", \"the\", \"Apple\", \"store\", \"but\", \"I\", \"do\", \"_n't\", \"see\", \"any\", \"apple\", \"_s\", \".\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained('gpt2-medium')\n",
    "\n",
    "def tokenize_conversation(csv_row):\n",
    "  return tokenizer(csv_row['Conversation'], truncation=True)\n",
    "tokenized_dataset = dataset.map(tokenize_conversation, batched=True, remove_columns=dataset['train'].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create blocks of tokens for training\n",
    "block_size = 256\n",
    "def group_texts(examples):\n",
    "  concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "  total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "  total_length = (total_length // block_size) * block_size\n",
    "  result = {\n",
    "    k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "    for k, t in concatenated_examples.items()\n",
    "  }\n",
    "  result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "  return result\n",
    "\n",
    "lm_dataset = tokenized_dataset.map(group_texts, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data collator for padding and data preparation before we start training\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the GPT model\n",
    "from transformers import AutoModelForCausalLM\n",
    "model = AutoModelForCausalLM.from_pretrained('gpt2-medium')\n",
    "# you can also try gpt2-large or gpt2-xl if you have the hardware for it.\n",
    "# this will need a pretty big GPU! Google Colab can only go up to gpt2-medium."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the model\n",
    "from transformers import TrainingArguments, Trainer\n",
    "training_args = TrainingArguments(\n",
    "  output_dir=\"dialogue-model\",\n",
    "  evaluation_strategy=\"epoch\",\n",
    "  learning_rate=2e-5,\n",
    "  weight_decay=0.01,\n",
    "  num_train_epochs=1, # this is how many times we go through the entire dataset. try 2 if you have a lot of time.\n",
    "  per_device_train_batch_size=4,\n",
    "  per_device_eval_batch_size=8\n",
    ")\n",
    "\n",
    "torch.cuda.empty_cache() # get the GPU ready for training\n",
    "trainer = Trainer(\n",
    "  model=model,\n",
    "  args=training_args,\n",
    "  train_dataset=lm_dataset['train'],\n",
    "  eval_dataset=lm_dataset['test'],\n",
    "  data_collator=data_collator\n",
    ")\n",
    "\n",
    "trainer.train() # this will take a while! about 15-20 minutes for me on Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text generation pipeline to see the trained model in action\n",
    "from transformers import pipeline\n",
    "generator = pipeline('text-generation', model=model)\n",
    "\n",
    "def generate_messages(prompt='', num=10, max_length=128):\n",
    "  outputs = generator(prompt, num_return_sequences=num, max_new_tokens=max_length)\n",
    "  for output in outputs:\n",
    "    print(\"-\"*20)\n",
    "    text = output['generated_text']\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final function. choose prompts and parameters as desired.\n",
    "# an example if you want a conversation on a particular topic:\n",
    "# A: what do you think about large language models?\n",
    "generate_messages(\": \", num=2, max_length=256)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
